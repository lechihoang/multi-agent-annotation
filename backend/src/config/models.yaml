# ===========================================
# Model Configurations for Multi-Agent System
# ===========================================

# Groq Models (High-performance, fast inference)
groq:
  intent_classification:
    model: "deepseek-r1-distill-llama-70b"
    temperature: 0.1
    max_tokens: 1024
    description: "DeepSeek R1 for complex reasoning and intent classification"

  entity_extraction:
    model: "llama-3.3-70b-versatile"
    temperature: 0.0
    max_tokens: 2048
    description: "Llama 70B for accurate entity extraction"

  fallback:
    model: "llama-3.1-8b-instant"
    temperature: 0.1
    max_tokens: 1024
    description: "Fast fallback model for rate limiting"

# HuggingFace Models (Free tier, pattern matching)
huggingface:
  faq_matching:
    model: "meta-llama/Llama-3.2-8B-Instruct"
    provider: "hf-inference"
    temperature: 0.1
    max_tokens: 1024
    description: "Llama 8B for FAQ and pattern matching"

  entity_extraction:
    model: "meta-llama/Llama-3.2-8B-Instruct"
    provider: "hf-inference"
    temperature: 0.0
    max_tokens: 2048
    description: "Alternative for entity extraction when Groq unavailable"

# Ollama Models (Local, zero cost)
ollama:
  validation:
    model: "llama3.2"
    temperature: 0.0
    max_tokens: 512
    description: "Local validation and preprocessing"

  simple_classification:
    model: "llama3.2"
    temperature: 0.1
    max_tokens: 256
    description: "Simple classification tasks"

  fallback:
    model: "llama3.2"
    temperature: 0.1
    max_tokens: 1024
    description: "Ultimate fallback when cloud APIs fail"

# ===========================================
# Agent Configurations
# ===========================================

agents:
  router:
    name: "router_agent"
    description: "Routes tasks to appropriate APIs based on complexity"
    tier: 1

  intent:
    name: "intent_agent"
    description: "Classifies user intents"
    tier: 2
    weight: 0.35
    primary_provider: "groq"
    primary_model: "deepseek-r1-distill-llama-70b"
    fallback_provider: "huggingface"
    fallback_model: "meta-llama/Llama-3.2-8B-Instruct"

  entity:
    name: "entity_agent"
    description: "Extracts named entities"
    tier: 2
    weight: 0.35
    primary_provider: "groq"
    primary_model: "llama-3.3-70b-versatile"
    fallback_provider: "huggingface"
    fallback_model: "meta-llama/Llama-3.2-8B-Instruct"

  faq:
    name: "faq_agent"
    description: "Matches against FAQ patterns"
    tier: 2
    weight: 0.30
    primary_provider: "huggingface"
    primary_model: "meta-llama/Llama-3.2-8B-Instruct"
    fallback_provider: "ollama"
    fallback_model: "llama3.2"

  judge:
    name: "judge_agent"
    description: "Aggregates votes and makes decisions"
    tier: 3

# ===========================================
# Complexity Routing Rules
# ===========================================

routing:
  high_complexity:
    description: "Complex tasks requiring deep reasoning"
    indicators:
      - "multi-step reasoning"
      - "ambiguous context"
      - "domain-specific terminology"
      - "long text (>500 chars)"
    primary_provider: "groq"
    models:
      - "deepseek-r1-distill-llama-70b"
      - "llama-3.3-70b-versatile"

  medium_complexity:
    description: "Standard annotation tasks"
    indicators:
      - "clear context"
      - "common patterns"
      - "moderate length (100-500 chars)"
    primary_provider: "groq"
    fallback_provider: "huggingface"
    models:
      - "llama-3.3-70b-versatile"
      - "meta-llama/Llama-3.2-8B-Instruct"

  low_complexity:
    description: "Simple, pattern-based tasks"
    indicators:
      - "short text (<100 chars)"
      - "FAQ-like queries"
      - "simple classification"
    primary_provider: "huggingface"
    fallback_provider: "ollama"
    models:
      - "meta-llama/Llama-3.2-8B-Instruct"
      - "llama3.2"

# ===========================================
# Rate Limiting Configuration
# ===========================================

rate_limits:
  groq:
    requests_per_minute: 30
    tokens_per_minute: 100000

  huggingface:
    requests_per_minute: 60
    tokens_per_minute: 50000

  ollama:
    requests_per_minute: -1  # Unlimited (local)
    tokens_per_minute: -1
